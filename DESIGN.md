DESIGN CHOICES:

The search page you have the option to select locations from the dropdown menu when you select a location this submits a post request to data. you must select one of the locations if not it renders the search page again. once you select the location this location is then used to select a website from the database. Once this website is selected it's html content is retrieved using requests and then this is run through a python libary called beautiful soup which helps extract the relevant data that is needed to be displayed in the data html. 

The website scrapping is where I ran into alot of trouble and is where I spent most of my time for this web-app. Alot of the data, that the websites showing avalanche reports present,  use javascript. the requests libary can't get javascript content in html. There are 2 solutions to this problem. 1- I could retrieve the javascript request that is run in the website and then run the same request in my python code. Unfortunately, when looking at the JS request it makes a request to a local server that I do not have access to and as a result I could not do this as my solution. the second option was to use a liabary called selenium which uses an external browser called chromium. this allowed me to make javascript requests and get the neccessary content. However, when thinking about the purposes of this web-app I realised that this would not be an appplicable method. This is becasue if other people would want to use the web-app they would all need to download chromium and put it in the same path as I currently have it set to. As a result, I decided to change back to using requests and not access any content that uses javascript. In the future, if I could find a way to simulate the javascript request by getting access to the content on the internal server this would probably be the best option to try solve this problem. 

the data page originally would display the grade and type without a time stamp. This I realised would be confusing for the user since they would not be able to tell when that data was from and this is important as avalanche reports can change alot from day to day. The reason I did not want to retrieve the data from every location each time I render the data page was beacuse the aim of the website is for it to be a global database and so when I have added several more locations to get it to scrape 100s of websites would take a very long time. as a solution I choose to introduce a timestamp column which shows when it was last updated. The timestamp and stats for the location get updated everytime you search for a location in the search tab. 

the avalanche database- I decided to use an SQL database instead of just scraping and outputting directly what the website is saying. I made this decision because sometimes websites can de down and I thought that when I expand this to include even more locations I would preferably have some data to output as even though avalanche reports can vary alot from day to day unless there is a significant change in conditions the avalanche report stays fairly similar from day to day. the avalanche database includes 6 columns, id, location, grade, type, location and timestamp. the inclusion of these have been explained above. grade is chosen becasue it is arguably the value which tells you the most. I included the type which gives a bit more information about the type of avalanche to be careful for, I do not display direction as it requires javascript which for the moment can't be accessed.
